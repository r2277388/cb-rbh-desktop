{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d20fdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "412f7ede",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The file at 'E:\\My Drive\\Colab Notebooks\\cb_forecasting\\df_pickle.pkl' uses an unsupported pickle protocol. Ensure that your Python version supports this protocol, or recreate the file with a lower protocol version.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-6a9b0b3ce6ca>\u001b[0m in \u001b[0;36mload_dataframe\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0mdf_pickled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[0;32m    216\u001b[0m                     \u001b[1;31m# expected \"IO[bytes]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mexcs_to_catch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: unsupported pickle protocol: 5",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-6a9b0b3ce6ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf_pickled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mdf_pickled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-6a9b0b3ce6ca>\u001b[0m in \u001b[0;36mload_dataframe\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"The file at '{file_path}' could not be unpickled. The file may be corrupted or incompatible.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         raise ValueError(f\"The file at '{file_path}' uses an unsupported pickle protocol. \"\n\u001b[0m\u001b[0;32m     18\u001b[0m                          \"Ensure that your Python version supports this protocol, or recreate the file with a lower protocol version.\")\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The file at 'E:\\My Drive\\Colab Notebooks\\cb_forecasting\\df_pickle.pkl' uses an unsupported pickle protocol. Ensure that your Python version supports this protocol, or recreate the file with a lower protocol version."
     ]
    }
   ],
   "source": [
    "# Path to the pickle file\n",
    "file_path = 'E:\\\\My Drive\\\\Colab Notebooks\\\\cb_forecasting\\\\df_pickle.pkl'\n",
    "\n",
    "# with open(file_path, 'rb') as file:\n",
    "#     df_pickled = pickle.load(file)\n",
    "\n",
    "# Load the DataFrame from the pickle file using a with clause\n",
    "def load_dataframe(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            df_pickled = pd.read_pickle(file)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"The file at '{file_path}' does not exist. Please check the file path and try again.\")\n",
    "    except pickle.UnpicklingError:\n",
    "        raise pickle.UnpicklingError(f\"The file at '{file_path}' could not be unpickled. The file may be corrupted or incompatible.\")\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"The file at '{file_path}' uses an unsupported pickle protocol. \"\n",
    "                         \"Ensure that your Python version supports this protocol, or recreate the file with a lower protocol version.\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"An unexpected error occurred while loading the pickle file: {e}\")\n",
    "    return df_pickled\n",
    "\n",
    "df_pickled = load_dataframe(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641fc241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'ds' to datetime if it's not already\n",
    "df_pickled['ds'] = pd.to_datetime(df_pickled['ds'])\n",
    "\n",
    "# Determine the date two weeks ago from the latest date in your data\n",
    "latest_date = df_pickled['ds'].max()\n",
    "two_weeks_ago = latest_date - pd.Timedelta(weeks=2)\n",
    "\n",
    "# Filter the DataFrame for the last two weeks\n",
    "df_last_two_weeks = df_pickled[df_pickled['ds'] >= two_weeks_ago]\n",
    "\n",
    "# Group by date and count records\n",
    "daily_counts = df_last_two_weeks.groupby('ds').size().reset_index(name='record_count')\n",
    "\n",
    "# Display the result\n",
    "print(daily_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c4e1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_counts.record_count.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b6d2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current year and month\n",
    "current_year = dt.datetime.now().year\n",
    "current_month = dt.datetime.now().month\n",
    "\n",
    "# Format for previous month\n",
    "previous_year, previous_month = (current_year - int(current_month == 1), 12 if current_month == 1 else current_month - 1)\n",
    "\n",
    "# Format periods as 'yyyymm'\n",
    "current_period = f\"{current_year}{current_month:02d}\"\n",
    "previous_period = f\"{previous_year}{previous_month:02d}\"\n",
    "\n",
    "# Remove data for these periods\n",
    "df_pickled = df_pickled[~df_pickled['period'].isin([current_period, previous_period])]\n",
    "\n",
    "current_period, previous_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78903370",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_saldet = f'''\n",
    "SELECT\n",
    "    sd.period\n",
    "    ,CASE\n",
    "        WHEN DATEPART(WEEKDAY, sd.TRX_DATE) = 1 THEN DATEADD(DAY, -2, sd.TRX_DATE) -- Adjusting Sunday to Friday\n",
    "        WHEN DATEPART(WEEKDAY, sd.TRX_DATE) = 7 THEN DATEADD(DAY, -1, sd.TRX_DATE) -- Adjusting Saturday to Friday\n",
    "        ELSE sd.TRX_DATE\n",
    "    END AS [ds]\n",
    "    ,CASE\n",
    "        WHEN LEFT(i.PUBLISHING_GROUP,3) = 'BAR' THEN 'BAR'\n",
    "        WHEN i.PUBLISHER_CODE = 'Princeton' THEN 'CPA'\n",
    "        ELSE i.PUBLISHING_GROUP\n",
    "    END pgrp\n",
    "    ,ssr_row.Description ssr\n",
    "    ,CASE\n",
    "        WHEN ssr_row.SSRRowID IN('32','146') then 'Consignment'\n",
    "        WHEN ssr_row.SSRRowID IN('6') then 'Amazon'\n",
    "        ELSE chan.Description\n",
    "    END channel\n",
    "    ,CASE\n",
    "        WHEN [dbo].[fnFrontBackListCode](i.AMORTIZATION_DATE,sd.TRX_DATE) IN('A','R') THEN 'F'\n",
    "        ELSE 'B'\n",
    "    END flbl\n",
    "    ,SUM(CASE \n",
    "            WHEN i.PUBLISHER_CODE = 'Princeton' AND YEAR(sd.TRX_DATE) > 2022 THEN 0 \n",
    "            ELSE sd.REVENUE_AMOUNT \n",
    "        END) AS [y]\n",
    "FROM\n",
    "    ebs.Sales sd\n",
    "    INNER JOIN ssr.SalesSSRRow stie on stie.CUSTOMER_TRX_LINE_ID = sd.CUSTOMER_TRX_LINE_ID\n",
    "    INNER JOIN ssr.SSRRow ssr_row on ssr_row.SSRRowID= stie.SSRRowID\n",
    "    INNER JOIN ssr.SubChannel sub on sub.SubChannelID = ssr_row.SubChannelID\n",
    "    INNER JOIN ssr.Channel chan on chan.ChannelID = sub.ChannelID\n",
    "    INNER JOIN ebs.Item i ON i.ITEM_ID = sd.ITEM_ID\n",
    "WHERE\n",
    "    sd.PERIOD >= ?\n",
    "    AND sd.INVOICE_LINE_TYPE = 'SALE'\n",
    "    AND cbq2.dbo.fnSaleTypeCode(SD.AR_TRX_TYPE_ID) = 'N'\n",
    "    AND i.PRICE_AMOUNT <> 0\n",
    "    AND i.PUBLISHER_CODE IN('Chronicle','Princeton')\n",
    "    AND i.PRODUCT_TYPE IN ('BK', 'FT')\n",
    "GROUP BY\n",
    "    sd.period\n",
    "    ,CASE \n",
    "        WHEN DATEPART(WEEKDAY, sd.TRX_DATE) = 1 THEN DATEADD(DAY, -2, sd.TRX_DATE) -- Adjusting Sunday to Friday\n",
    "        WHEN DATEPART(WEEKDAY, sd.TRX_DATE) = 7 THEN DATEADD(DAY, -1, sd.TRX_DATE) -- Adjusting Saturday to Friday\n",
    "        ELSE sd.TRX_DATE\n",
    "    END\n",
    "    ,CASE\n",
    "        WHEN LEFT(i.PUBLISHING_GROUP,3) = 'BAR' THEN 'BAR'\n",
    "        WHEN i.PUBLISHER_CODE = 'Princeton' THEN 'CPA'\n",
    "        ELSE i.PUBLISHING_GROUP\n",
    "    END\n",
    "    ,ssr_row.Description\n",
    "    ,CASE\n",
    "        WHEN ssr_row.SSRRowID IN('32','146') then 'Consignment'\n",
    "        WHEN ssr_row.SSRRowID IN('6') then 'Amazon'\n",
    "        ELSE chan.Description\n",
    "    END\n",
    "    ,CASE\n",
    "        WHEN [dbo].[fnFrontBackListCode](i.AMORTIZATION_DATE,sd.TRX_DATE) IN('A','R') THEN 'F'\n",
    "        ELSE 'B'\n",
    "    END\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d32e342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can be used to run the SQL query for any period after the parameter 'period'\n",
    "def query_data(period = previous_period):\n",
    "    # SQL python connection to our server\n",
    "    conn = pyodbc.connect('Driver={SQL Server};'\n",
    "                          'Server=sql-2-db;'\n",
    "                          'Database=CBQ2;')\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    df = pd.read_sql_query(query_saldet\n",
    "                           ,conn\n",
    "                           ,dtype={'period':'category'\n",
    "                                   ,'pgrp':'category'\n",
    "                                   ,'channel':'category'\n",
    "                                   ,'ssr':'category'\n",
    "                                   ,'flbl':'category'\n",
    "                                   ,'y':'float64'\n",
    "                                    }\n",
    "                            ,parse_dates=['ds']\n",
    "                            ,params=[previous_period]\n",
    "                            )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918dfaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just querying the last two months of data\n",
    "df_additional = query_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7dded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the current pickled dataset\n",
    "df_pickled_rows = df_pickled.shape[0]\n",
    "print(f'{df_pickled_rows: ,.0f}')\n",
    "\n",
    "df_pickled.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b39c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_additional_rows = df_additional.shape[0]\n",
    "print(f'{df_additional_rows: ,.0f}')\n",
    "df_additional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e48e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combo = pd.concat([df_pickled,df_additional],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a14a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combo['period'] = df_combo['period'].astype('category')\n",
    "df_combo['ssr'] = df_combo['ssr'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71e9326",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9825b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combo.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10b4906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_combination():\n",
    "    if df_combo.shape[0] == df_additional.shape[0] + df_pickled.shape[0]:\n",
    "        print(f'Concatenation Worked!')\n",
    "        max_date = df_combo.ds.max()\n",
    "        formatted_date = max_date.strftime('%Y-%m-%d')\n",
    "        print(f'Last Date: {formatted_date}')\n",
    "    else:\n",
    "        print(f'Check that the additional periods were correct added')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4552b524",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check_combination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614106a8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_combo.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f30ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder where you want to save the pickle file\n",
    "folder_path = 'E:\\\\My Drive\\\\Colab Notebooks\\\\cb_forecasting\\\\'\n",
    "\n",
    "# Check if the folder exists\n",
    "if not os.path.exists(folder_path):\n",
    "    print(f\"The folder '{folder_path}' does not exist. Please check the path.\")\n",
    "else:\n",
    "    # Filename for the pickle file\n",
    "    filename = 'df_pickle.pkl'\n",
    "\n",
    "    # Full path for saving the file\n",
    "    full_path = os.path.join(folder_path, filename)\n",
    "\n",
    "    # Save the DataFrame to a pickle file\n",
    "    df_combo.to_pickle(full_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
